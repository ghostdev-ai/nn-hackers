{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A hacker's guide to Neural Networks\n",
    "\n",
    "This notebook is a theoretical approach to breaking down the inner workings of neural networks into digestable parts to help gain a better inuition for how neural networks work. A neural network is twofold **forward propagation** and **backpropagation**. We'll break down forward propagation and backpropagation exploring each concept indepth including their biological inspiration and mathematical understanding; slowly building up to the bigger idea. I've found exploring each idea in-depth helps to better understand how these systems work. In essences, neural networks are a black box and the goal is to demystify these systems so you can better understand how they work.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](./images/Perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.2, 5.1, 2.1] \n",
    "weights = [3.1, 2.1, 8.7]\n",
    "bias = 3\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7\n"
     ]
    }
   ],
   "source": [
    "# Dot product - multiplies each scalar element-wise then sums those scalars together\n",
    "output = np.dot(weights, inputs) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi-Linear_Perceptron](./images/Multi-Linear_Perceptron.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78599378 -2.74494004 -0.92044729]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "inputs = np.random.randn(5)\n",
    "weights = np.random.randn(3, 5)\n",
    "biases = np.random.randn(3)\n",
    "\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.17115625, -1.16803526, -0.82160782,  0.92183037, -0.33256601])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.17115625, -1.16803526, -0.82160782,  0.92183037, -0.33256601]),\n",
       " (5,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.1495665 , -0.14756567, -1.19343778,  0.52912856,  0.84744265],\n",
       "        [-0.33508691,  1.40474206,  1.35984153, -0.29074424,  0.1959536 ],\n",
       "        [ 0.87985141, -0.13661696, -1.098662  , -0.99735146, -0.06985282]]),\n",
       " (3, 5))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0\n",
      "6.0\n",
      "6.0\n",
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "print(xw0, xw1, xw2)\n",
    "\n",
    "# Adding weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "\n",
    "dsum_dxw1 = 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "\n",
    "dsum_dxw2 = 1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "\n",
    "\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "\n",
    "dmul_dw0 = x[0]\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "\n",
    "dmul_dx1 = w[1]\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "\n",
    "dmul_dw1 = x[1]\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "\n",
    "dmul_dx2 = w[2]\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n",
      "[-3.001, -0.998, 1.997] 0.999\n"
     ]
    }
   ],
   "source": [
    "print(w, b)\n",
    "\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db\n",
    "\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.985\n"
     ]
    }
   ],
   "source": [
    "# Multiply inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[.2, .8, -.5, 1],\n",
    "                    [.5, -.91, .26, -.5],\n",
    "                    [-.26, -.27, .17, .87]]).T\n",
    "\n",
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron \n",
    "dx0 = sum(weights[0]*dvalues[0])\n",
    "dx1 = sum(weights[1]*dvalues[0])\n",
    "dx2 = sum(weights[2]*dvalues[0])\n",
    "dx3 = sum(weights[3]*dvalues[0])\n",
    "\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3, 4)\n",
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "print(dvalues[0].shape, weights.T.shape)\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) (3, 4)\n",
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[.2, .8, -.5, 1],\n",
    "                    [.5, -.91, .26, -.5],\n",
    "                    [-.26, -.27, .17, .87]]).T\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "print(dvalues.shape, weights.T.shape)\n",
    "\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2.],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neuron)\n",
    "biases = np.array([[2, 3, .5]])\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims \n",
    "# since this by default will produce a plain list - \n",
    "# we explained this in the chapter 4\n",
    "print(dvalues.shape)\n",
    "\n",
    "# \"keepdims\" lets us keep the gradient as a row vector - recall the shape of biases array.\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "\n",
    "print(drelu)\n",
    "\n",
    "# The chain rule\n",
    "drelu *= dvalues\n",
    "\n",
    "print(drelu)\n",
    "\n",
    "\n",
    "# ReLU activation's derivative\n",
    "# with the chain rule applied\n",
    "drelu = dvalues.copy()\n",
    "drelu[z <= 0] = 0\n",
    "\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2.],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[.2, .8, -.5, 1],\n",
    "                    [.5, -.91, .26, -.5],\n",
    "                    [-.26, -.27, .17, .87]]).T\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neuron)\n",
    "biases = np.array([[2, 3, .5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases  # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs)  # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "# ReLU activation - simulates derivative with respect to input values\n",
    "# from next layer passed to current layer during backpropagation\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "# Dense layer\n",
    "# dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list - \n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "# Update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
